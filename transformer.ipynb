{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5accd3-2791-4801-b773-72e0f9071b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coffeetumbler/anaconda3/envs/ano_trans/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_model_summary\n",
    "\n",
    "from models.transformer import get_transformer_encoder\n",
    "from models.anomaly_transformer import get_anomaly_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33712e9c-716e-45aa-a54b-f2c7c7633cc4",
   "metadata": {},
   "source": [
    "## Check Transformer Encoder (before adding relative position embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7acd25cd-5acc-442e-ae37-cff5f930de90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "    EncoderLayer-1      [10, 512, 512]       3,152,384       3,152,384\n",
      "    EncoderLayer-2      [10, 512, 512]       3,152,384       3,152,384\n",
      "    EncoderLayer-3      [10, 512, 512]       3,152,384       3,152,384\n",
      "    EncoderLayer-4      [10, 512, 512]       3,152,384       3,152,384\n",
      "    EncoderLayer-5      [10, 512, 512]       3,152,384       3,152,384\n",
      "    EncoderLayer-6      [10, 512, 512]       3,152,384       3,152,384\n",
      "=======================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_transformer_encoder(positional_encoding=None)\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(10, 512, 512), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ff96ab-4a11-474d-9a0b-69d80c958343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "                     Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "======================================================================================\n",
      "   SinusoidalPositionalEncoding-1      [10, 512, 512]               0               0\n",
      "                   EncoderLayer-2      [10, 512, 512]       3,152,384       3,152,384\n",
      "                   EncoderLayer-3      [10, 512, 512]       3,152,384       3,152,384\n",
      "                   EncoderLayer-4      [10, 512, 512]       3,152,384       3,152,384\n",
      "                   EncoderLayer-5      [10, 512, 512]       3,152,384       3,152,384\n",
      "                   EncoderLayer-6      [10, 512, 512]       3,152,384       3,152,384\n",
      "                   EncoderLayer-7      [10, 512, 512]       3,152,384       3,152,384\n",
      "======================================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_transformer_encoder(positional_encoding='Sinusoidal')\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(10, 512, 512), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076044e6-5bf3-4ae5-acd3-ce7a1778b9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "                  Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "===================================================================================\n",
      "   AbsolutePositionEmbedding-1      [10, 512, 512]         262,144         262,144\n",
      "                EncoderLayer-2      [10, 512, 512]       3,152,384       3,152,384\n",
      "                EncoderLayer-3      [10, 512, 512]       3,152,384       3,152,384\n",
      "                EncoderLayer-4      [10, 512, 512]       3,152,384       3,152,384\n",
      "                EncoderLayer-5      [10, 512, 512]       3,152,384       3,152,384\n",
      "                EncoderLayer-6      [10, 512, 512]       3,152,384       3,152,384\n",
      "                EncoderLayer-7      [10, 512, 512]       3,152,384       3,152,384\n",
      "===================================================================================\n",
      "Total params: 19,176,448\n",
      "Trainable params: 19,176,448\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_transformer_encoder(positional_encoding='Absolute')\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(10, 512, 512), show_input=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205db6c-6272-4b3e-b08e-c628f36791ce",
   "metadata": {},
   "source": [
    "## Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab5a5aed-deb3-4fce-a629-482aa06c5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  8  7  6  5  4  3  2  1  0]\n",
      " [10  9  8  7  6  5  4  3  2  1]\n",
      " [11 10  9  8  7  6  5  4  3  2]\n",
      " [12 11 10  9  8  7  6  5  4  3]\n",
      " [13 12 11 10  9  8  7  6  5  4]\n",
      " [14 13 12 11 10  9  8  7  6  5]\n",
      " [15 14 13 12 11 10  9  8  7  6]\n",
      " [16 15 14 13 12 11 10  9  8  7]\n",
      " [17 16 15 14 13 12 11 10  9  8]\n",
      " [18 17 16 15 14 13 12 11 10  9]]\n"
     ]
    }
   ],
   "source": [
    "# Set position index\n",
    "max_seq_len = 10  # Maximum sequence length example\n",
    "coords_h = np.arange(max_seq_len)\n",
    "coords_w = np.arange(max_seq_len-1, -1, -1)\n",
    "coords = coords_h[:, None] + coords_w[None, :]\n",
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "075bd69e-c005-4ba1-a6d8-e430bd4f19db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table :\n",
      "0 : [0.14965618 0.16788995]\n",
      "1 : [0.9504504  0.10648322]\n",
      "2 : [0.3820259  0.26660728]\n",
      "3 : [0.5858676 0.9569891]\n",
      "4 : [0.30918825 0.55566347]\n",
      "5 : [0.5504618 0.6121726]\n",
      "6 : [0.42828113 0.7249437 ]\n",
      "7 : [0.69184756 0.5860372 ]\n",
      "8 : [0.81165934 0.7614068 ]\n",
      "9 : [0.147039   0.13363111]\n",
      "10 : [0.93855715 0.44990999]\n",
      "11 : [0.8766833 0.7013564]\n",
      "12 : [0.8035398 0.8917592]\n",
      "13 : [0.7712254  0.45665962]\n",
      "14 : [0.54262006 0.34000492]\n",
      "15 : [0.9725495 0.4922533]\n",
      "16 : [0.8967813  0.36961406]\n",
      "17 : [0.82937473 0.42501265]\n",
      "18 : [0.6240979  0.92276263]\n"
     ]
    }
   ],
   "source": [
    "# Relative position embedding\n",
    "n_head = 2  # Number of heads example\n",
    "relative_position_embedding_table = torch.rand(2*max_seq_len-1, n_head)\n",
    "print('table :')\n",
    "for i, bias in enumerate(relative_position_embedding_table):\n",
    "    print(i, ':', bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21e6bf8b-990f-411c-9469-2528e7f591c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel pos :\n",
      "<head 1>\n",
      "0 : [0.147039   0.81165934 0.69184756 0.42828113 0.5504618  0.30918825\n",
      " 0.5858676  0.3820259  0.9504504  0.14965618]\n",
      "1 : [0.93855715 0.147039   0.81165934 0.69184756 0.42828113 0.5504618\n",
      " 0.30918825 0.5858676  0.3820259  0.9504504 ]\n",
      "2 : [0.8766833  0.93855715 0.147039   0.81165934 0.69184756 0.42828113\n",
      " 0.5504618  0.30918825 0.5858676  0.3820259 ]\n",
      "3 : [0.8035398  0.8766833  0.93855715 0.147039   0.81165934 0.69184756\n",
      " 0.42828113 0.5504618  0.30918825 0.5858676 ]\n",
      "4 : [0.7712254  0.8035398  0.8766833  0.93855715 0.147039   0.81165934\n",
      " 0.69184756 0.42828113 0.5504618  0.30918825]\n",
      "5 : [0.54262006 0.7712254  0.8035398  0.8766833  0.93855715 0.147039\n",
      " 0.81165934 0.69184756 0.42828113 0.5504618 ]\n",
      "6 : [0.9725495  0.54262006 0.7712254  0.8035398  0.8766833  0.93855715\n",
      " 0.147039   0.81165934 0.69184756 0.42828113]\n",
      "7 : [0.8967813  0.9725495  0.54262006 0.7712254  0.8035398  0.8766833\n",
      " 0.93855715 0.147039   0.81165934 0.69184756]\n",
      "8 : [0.82937473 0.8967813  0.9725495  0.54262006 0.7712254  0.8035398\n",
      " 0.8766833  0.93855715 0.147039   0.81165934]\n",
      "9 : [0.6240979  0.82937473 0.8967813  0.9725495  0.54262006 0.7712254\n",
      " 0.8035398  0.8766833  0.93855715 0.147039  ]\n"
     ]
    }
   ],
   "source": [
    "relative_position_embedding = relative_position_embedding_table[coords.flatten()].view(max_seq_len, max_seq_len, -1)\n",
    "relative_position_embedding = relative_position_embedding.permute(2, 0, 1).contiguous()\n",
    "print('rel pos :')\n",
    "print('<head 1>')\n",
    "for i, bias in enumerate(relative_position_embedding[0]):\n",
    "    print(i, ':', bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9514ef9-68ec-4839-95d5-7172a925bacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0, 10,  9,  8,  7,  6,  5,  4,\n",
       "        3,  2,  1, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2, 12, 11, 10,  9,\n",
       "        8,  7,  6,  5,  4,  3, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4, 14,\n",
       "       13, 12, 11, 10,  9,  8,  7,  6,  5, 15, 14, 13, 12, 11, 10,  9,  8,\n",
       "        7,  6, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7, 17, 16, 15, 14, 13,\n",
       "       12, 11, 10,  9,  8, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5acb6b4-2686-4c9a-8202-ed19c7c68c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "    EncoderLayer-1      [10, 512, 512]       3,160,568       3,160,568\n",
      "    EncoderLayer-2      [10, 512, 512]       3,160,568       3,160,568\n",
      "    EncoderLayer-3      [10, 512, 512]       3,160,568       3,160,568\n",
      "    EncoderLayer-4      [10, 512, 512]       3,160,568       3,160,568\n",
      "    EncoderLayer-5      [10, 512, 512]       3,160,568       3,160,568\n",
      "    EncoderLayer-6      [10, 512, 512]       3,160,568       3,160,568\n",
      "=======================================================================\n",
      "Total params: 18,963,408\n",
      "Trainable params: 18,963,408\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check Transformer encoder with relative position embedding\n",
    "model = get_transformer_encoder(positional_encoding=None, relative_position_embedding=True)\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(10, 512, 512), show_input=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add837ae-2a52-47ac-a302-a22e2772cea5",
   "metadata": {},
   "source": [
    "## Check Anomaly Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618ceb6d-98b2-4c34-af5f-bbd615651eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "           Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "               Linear-1       [16, 100, 64]             704             704\n",
      "   TransformerEncoder-2       [16, 100, 64]         152,340         152,340\n",
      "               Linear-3      [16, 100, 256]          16,640          16,640\n",
      "                 GELU-4      [16, 100, 256]               0               0\n",
      "               Linear-5       [16, 100, 10]           2,570           2,570\n",
      "============================================================================\n",
      "Total params: 172,254\n",
      "Trainable params: 172,254\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Summary the example model.\n",
    "model = get_anomaly_transformer(d_data=10,\n",
    "                                d_embed=64,\n",
    "                                hidden_dim_rate=4.,\n",
    "                                max_seq_len=100,\n",
    "                                mask_token_rate=(0.05,0.15),\n",
    "                                positional_encoding=None,\n",
    "                                relative_position_embedding=True,\n",
    "                                transformer_n_layer=3,\n",
    "                                transformer_n_head=4,\n",
    "                                dropout=0.1)\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(16, 100, 10), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfa6633-f449-487b-a484-93abc0f10414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 6, 14, 12, 8, 12, 10, 11, 10, 14]\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.0053, -0.0057, -0.0327,  0.0048,  0.0043, -0.0022, -0.0176, -0.0310,\n",
      "         -0.0053,  0.0069, -0.0078,  0.0106,  0.0044,  0.0415, -0.0048, -0.0029,\n",
      "          0.0119, -0.0117,  0.0170,  0.0006, -0.0185,  0.0253, -0.0154, -0.0067,\n",
      "         -0.0159, -0.0109,  0.0060, -0.0160,  0.0078,  0.0099,  0.0308,  0.0719,\n",
      "          0.0068, -0.0020, -0.0179,  0.0115,  0.0029,  0.0234, -0.0004,  0.0065,\n",
      "          0.0188, -0.0159, -0.0118, -0.0079, -0.0005, -0.0236, -0.0139,  0.0228,\n",
      "          0.0358,  0.0157, -0.0106,  0.0102, -0.0012,  0.0266,  0.0271, -0.0069,\n",
      "          0.0165, -0.0182, -0.0007,  0.0148,  0.0243,  0.0238, -0.0059, -0.0005]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Check model instances.\n",
    "print(model.mask_token_table[:10])\n",
    "print()\n",
    "print(model.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76cdc2e7-b92d-46bd-b7dd-25f3b2a9de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 2.413358211517334\n"
     ]
    }
   ],
   "source": [
    "# Check autograd functions.\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "\n",
    "x = torch.rand(16, 100, 10)\n",
    "y = model(x)\n",
    "loss = l1_loss(x, y)\n",
    "print('loss :', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6b6e57-2ff3-4e68-9400-8cead8c500c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2086, -0.2762,  0.2535,  1.0324,  0.1024,  0.0914,  0.9866, -0.5405,\n",
      "         0.3809, -0.0159,  0.4031,  0.1038,  0.8071, -0.4886, -0.3818, -0.1702,\n",
      "         1.0533,  0.5017, -0.2825, -0.7611, -0.3886,  0.7570,  0.8553, -0.0265,\n",
      "         0.0247, -0.4464,  0.1729, -0.0541,  0.0444,  0.1384, -0.3599, -0.4125,\n",
      "         0.0700,  0.4699, -0.2983, -0.3459,  0.1212,  0.3143, -0.2653,  0.5975,\n",
      "         0.3497,  0.0197,  0.1166, -0.0435, -0.5129,  0.4491, -0.4168,  0.1174,\n",
      "        -0.2196, -0.2135, -0.0068, -1.2207,  0.1179, -0.0373, -0.1633, -0.3557,\n",
      "        -0.5474, -0.8561,  0.5296,  0.0873, -0.8980, -0.6438,  0.2070, -0.0990])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(model.linear_embedding.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9e17e3-3b9a-4dae-934c-c1ed6e2d9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5545, -0.7614,  0.4503,  0.7524,  0.0604, -0.4823,  0.4422,  0.8438,\n",
      "          1.0116,  0.7272,  0.1171,  0.6688,  0.7854,  0.0084,  0.6613,  0.5270,\n",
      "          0.6445,  0.1250,  1.0520, -1.3416,  1.2106,  0.8090,  0.1353,  0.4056,\n",
      "          0.4472, -1.2009, -0.8700, -1.2629,  0.5274, -0.5598, -0.2111,  0.1449,\n",
      "          0.7595,  1.2007,  0.3101, -1.2605,  0.2475, -0.2101, -0.7676,  1.1885,\n",
      "          0.7706,  0.4547, -0.6664, -0.9339, -1.1345, -0.4515, -0.1796,  1.0323,\n",
      "         -0.5746, -0.0408,  0.2519, -0.4176,  0.1028, -1.4091, -0.1960, -1.2700,\n",
      "         -1.6909, -1.5927, -0.2550,  0.4100, -0.2361,  0.4618,  0.0729,  0.6517]])\n"
     ]
    }
   ],
   "source": [
    "print(model.mask_token.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc77fa1e-0b22-4528-8cab-a9d59bc3c2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7156e-05, -6.6179e-06,  2.1336e-06, -1.0048e-05],\n",
      "        [-2.6890e-05, -2.2301e-06,  1.1036e-05,  1.2869e-06],\n",
      "        [-2.6311e-05,  6.2818e-06,  9.8580e-06, -3.8717e-07],\n",
      "        [-1.3054e-05, -4.7467e-06,  1.4798e-05,  8.6596e-06],\n",
      "        [-2.4838e-05, -5.9972e-06,  2.3861e-05,  3.2242e-05],\n",
      "        [-2.4469e-05,  3.4905e-06,  2.1670e-05,  2.0059e-05],\n",
      "        [-3.3681e-05, -6.1303e-06,  4.3800e-05,  2.1766e-05],\n",
      "        [-2.6824e-05,  1.8091e-05,  6.1962e-05,  1.0274e-05],\n",
      "        [-4.8427e-05,  1.8547e-06,  7.4820e-05,  3.0014e-05],\n",
      "        [-4.1165e-05, -1.0241e-05,  9.8454e-05,  3.9157e-05]])\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer_encoder.encoder_layers[0].attention_layer.relative_position_embedding_table.grad[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee288b9-15bf-4f69-9027-bbbdef42d037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ano_trans",
   "language": "python",
   "name": "ano_trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
